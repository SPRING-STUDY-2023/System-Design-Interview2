# 5장 지표 모니터링 및 경보 시스템


## 1단계 문제 이해 및 설계 범위 확정

단순히 웹 서버 에러 로그, 액세스 로그 등을 수집하기 보다 인프라 수준에서 모니터링 가능한 시스템을 설계하기 위해 아래와 같은 설계 범위를 확정할 수 있다. 

- `사용 대상` : 회사 내부에서 사용할 시스템
- `수집할 지표`
    - 시스템 운영 지표
        - low level - CPU 부하, 메모리 사용률, 디스크 사용량
        - high level - 서버가처리하는 초당 요청 수, 웹 서버 프로세스 개수
    - 사업 지표는 대상이 아님
    - 에러 로그, 액세스 로그도 대싱이 아님
- `모니터링할 인프라 규모`
    - DAU 1억 명
    - 1000개의 서버 풀
    - 각 풀마다 100개의 서버 하드웨어
- `지표 데이터 보관 기간` : 1년
    - 장기 보관 전용 저장소로 옮길 때, 지표의 해상도를 낮추어도 좋음
    - New 데이터 - 7일 보관
    - 일주일 지난 데이터 - 1분 단위 데이터로 변환, 30일 보관
    - 7일+30일 지난 데이터 - 1시간 단위 데이터로 변환
- `경보 채널 지원`
    - 이메일
    - 전화
    - 페이저듀티(PagerDuty)
    - 웹훅 → HTTP 프로토콜을 지원하는 엔드포인트
- `기타 기능`
    - 분산 시스템 추적 → 필요 X

### 개략적 요구사항 및 가정

1. 대규모 인프라를 모니터링 해야 함
    - DAU 1억 명
    - 서버 풀 1000개, 풀당 서버 수 100개
        
        → 서버당 100개의 운영 지표를 수집한다면, 모니터링 해야 할 수는 1000만 개 (1000X100X100)
        
    - 데이터 보관 기간 1년
        - 7일(raw data) → 30일(1분) → 1년(1시간)
2. 모니터링 지표
    - CPU 사용률
    - 요청 수
    - 메모리 사용량
    - 메시지 큐 내외 메시지 수

### 비기능 요구사항

- `규모 확장성` : 늘어나는 지표 수, 경보의 양에 맞게 확장 가능
- `낮은 응답 지연` : 대시보드와 경보를 신속하게 처리 → 질의에 대한 낮은 응답 지연 보장
- `안정성` : 중요 경보를 놓치지 않도록
- `유연성` : 미래의 신기술에 쉽게 통합될 수 있도록 유연하게 변경 가능한 파이프라인 이용

### 고려사항이 아닌 것

- 로그 모니터링 - Elasticsearch, Logstash, Kibana 등
- 분산 시스템 추적 : 서비스에 대한 요청이 분산 시스템 내부를 어떻게 흘러 다니는지 추적하는 시스템 (요청이 이동할 때마다 데이터 수집)

## 2단계 개략적 설계안 제시 및 동의 구하기

### 시스템 구축을 위한 기본 사항

1. `데이터 수집` (data collection) : 여러 출처로부터 지표 데이터 수집
2. `데이터 전송` (data transmission) : 지표 데이터를 지표 모니터링 시스템으로 전송
3. `데이터 저장소` (data storage) : 전송되어 오는 데이터를 정리하고 저장
4. `경보` (alerting) : 밀려오는 데이터를 분석하고, 이상 징후를 감지하고, 경보를 발생시킴 to 다양한 통신 채널
5. `시각화` (visualization) : 데이터를 차트, 그래프 등으로 제공 for 패턴, 추이, 문제점을 쉽게 파악하기 위함

### 데이터 모델

> 지표 데이터는 통상 시계열(time series) 데이터 형태로 기록한다 →  값 집합에 따임스탬프가 붙은 형태
> 
> - 각 시계열 - 고유명 + label
> - 모든 시계열 데이터
>     
>     
>     | 이름 | 자료형 |
>     | --- | --- |
>     | 지표 이름 | 문자열 |
>     | 태그/레이블 집합 | <키:값> 쌍의 list |
>     | 지표 값 및 그 타임스탬프의 배열 | <값, 타임스탬프> 쌍의 array |

[사례1] 프로덕션에서 사용 중인 서버 인스턴스의 특정 시간대 CPU 부하를 알고 싶은 경우

- 시계열 데이터 = metric name + label (host:i631, env:prod) + 특정 시각에 측정된 지표 데이터

[사례2] 지난 10분간 특정 지역에 위치한 모든 웹 서버의 CPU 부하 평균값을 알고 싶은 경우

```
CPU.load host=webserver-01,region=us-west 1613707265 50
CPU.load host=webserver-01,region=us-west 1613707265 70
****CPU.load host=webserver-02,region=us-west 1613707265 32
...
****CPU.load host=webserver-02,region=us-west 1613707265 54
****CPU.load host=webserver-01,region=us-west 1613707265 83
```

- metric name(’CPU.load’) + label(’us-west’ 포함)인 데이터들을 저장소에서 가져와서
- 각 행의 마지막에 기록된 CPU 부하 수치의 평균값 구하기

→ Prometheus, OpenTSDB

### 데이터 접근 패턴

![Untitled](https://github.com/user-attachments/assets/3eea8874-d7c3-4416-9985-850e52be0cad)

- `x축` : 시간
- `y축` : 하나의 시계열 데이터 (metric name+label로 식별되는 값)

데이터가 많을수록 트래픽 빈도가 매우 높았음을 의미한다. 

### 데이터 저장소 시스템 ⭐

<aside>
❗ 모니터링 시스템 구축을 위한 저장소를 직접 설계하거나, MySQL 등의 범용 저장소를 사용하는 선택지는 배제한다.

***→ WHY?***

범용 데이터베이스 - 시계열 데이터를 처리할 수 있지만, 이 설계안이 감당하려는 부하 규모에 맞추려면 전문가 수준의 튜닝이 필요함

- 특히, 관계형 데이터베이스(e.g. MySQL)는 시계열 데이터 대상의 연산에 최적화 X (많은 양의 쓰기 연산이 지속적으로 발생하는 환경에 그리 좋은 성능을 보이진 않음)
- NoSQL(e.g. Cassandra, Bigtable)은 시계열 데이터 처리에 사용될 수는 있으나, 확장이 용이한 스키마 설계를 위해 해박한 내부 구조에 대한 배경지식이 필요함
</aside>

시장에 시계열 데이터에 최적화된 저장소 시스템은 많다. 

같은 양의 데이터를 더 적은 서버에 저장할 수 있는 이점도 있고, SQL보다 사용하기 쉬운 질의 인터페이스로 데이터 분석에도 용이하다.  

**시계열 Database 종류**

- OpenTSDB - 분산 시계열 데이터베이스로, Hadoop과 HBase에 기반 (하둡/HBase 클러스터를 구성 및 운영해야 하므로 다소 복잡)
- MetricsDB → X (구 트위터)에서 사용
- Timestream → Amazon에서 사용

[가장 인기 있는 DB]

- ⭐ **InfluxDB**
- ⭐ **Prometheus**

→ 특징

- 다량의 시계열 데이터를 저장
- 빠른 실시간 분석 지원
- 메모리 캐시와 디스크 저장소를 함께 사용
- 영속성 요건과 높은 성능 요구사항을 잘 만족함
    - ex. 8CPU 코어 + 32GM 램의 InfluxDB 서버 한 대로 초당 250,000회의 쓰기 연산 처리 가능
- 막대한 양의 시계열 데이터를 레이블(or 태그) 기준으로 집계하고 분석하는 기능을 제공
    - ex. InfluxDB - 레이블별 인덱스 구축
    - 이와 동시에 DB 과부하가 발생하지 않도록 각 레이블이 가질 수 있는 값의 가짓수(cardinality)가 낮아야 한다

<aside>
👀 본질적으로 시계열 데이터로 이루어진 지표 데이터를 시계열 DB에 저장할 수 있음을 설명하면 Good

</aside>

### 개략적 설계안

![Untitled](https://github.com/user-attachments/assets/df4a7f1a-87bd-43fa-b28c-42be0db1baca)

| 지표 출처 | 지표 데이터가 만들어지는 곳 (애플리케이션 서버, SQL 데이터베이스, 메시지 큐 등) |
| --- | --- |
| 지표 수집기 | 지표 데이터를 수집하고 시계열 데이터를 기록하는 역할 |
| 시계열 DB | 지표 데이터를 시계열 데이터 형태로 보관하는 저장소 |
| 질의 서비스 | 시계열 데이터베이스에 보관된 데이터를 질의하고 가져오는 과정을 돕는 서비스
*좋은 시계열 DB를 사용한다면, 해당 DB의 질의 인터페이스로 대체 가능 |
| 경보 시스템 | 경보를 받아야 하는 다양한 대상으로 경보 알림을 전송하는 역할을 하는 시스템 |
| 시각화 시스템 | 지표를 다양한 형태의 그래프/차트로 시각화 하는 기능 제공 |

## 3단계 상세 설계

### 지표 수집

- 때로 데이터가 소실되어도 아주 심각하지 않다
- 클라이언트가 성공적으로 데이터가 전송했는지 신경 쓰지 않아도 된다

![지표 수집 흐름](https://github.com/user-attachments/assets/a2fc36c5-e948-4621-9348-7b8317e039a5)

지표 수집 흐름

**`풀 모델`**

> 실행 중인 애플리케이션에서 주기적으로 지표 데이터를 가져오는 지표 수집기가 흐름의 중심
> 
- 지표 수집기는 데이터를 가져올 서비스 목록을 알고 있다.
    1. 서버 안에 모든 서비스 엔드포인트와 DNS/IP 정보를 담은 파일 두기
    2. 서비스 탐색 기술(e.g. etcd, Apache Zookeeper)을 활용해 수시로 변경되는 대규모 운영 환경에도 적용 가능
        
        SDS에 각 서비스마다의 가용성을 기록해두고, 엔드포인트 목록에 변화가 생기면 지표 수집기에 통보
        
- SDS 기술을 사용하는 풀 모델 기반 지표 수집 흐름 **[지표 수집기 관점]**
    1. SDS에서 서비스 엔드포인트 설정 메타데이터 목록을 가져온다
    2. 사전에 합의된 HTTP 엔드포인트에서 지표 데이터를 가져온다  *특정 클라이언트 라이브러리를 추가해 수집기에 노출되지 않도록 함
    3. 서비스 엔드포인트 목록의 변화를 통지 받기 위한 변경 이벤트 알림 콜백을 서비스 탐색 컴포넌트에 등록할 수 있다
- 메타데이터 종류
    - 지표 수집 주기(pulling interval)
    - IP 주소
    - timeout
    - retry parameter
- 한 대의 서버로는 다량의 데이터를 수집하기 어려우므로, 지표 수집기 서버 풀을 만들어야 감당 가능 → **안정 해시 링**을 이용해 메커니즘 구현

[대표적 사례] Prometheus

**`푸시 모델`**

> 지표 출처에 해당하는 서버(웹 서버, 데이터베이스 서버 등)가 직접 지표를 수집기에 전송하는 모델
> 
- 모니터링 대상 서버에 **수집 에이전트**라는 소프트웨어를 설치한다
    - **수집 에이전트**의 역할 - 해당 장비에서 실행되는 서비스가 생산하는 지표 데이터를 받아서 모아둔 후, 주기적으로 수집기에 전달 (간단한 지표는 직접 집계 가능)
- 데이터 집계는 수집기에 보내는 데이터 양을 줄이는 효과적인 방법이다
- 지표 수집기가 지표 데이터를 제때 처리하지 못하는 상황을 방지하려면, 지표 수집기 클러스터 자체가 auto scaling이 가능하게 하고 그 앞에 로드밸런서를 두면 된다.

[대표적 사례] Amazon CloudWatch, Graphite

<aside>
🌟 장단점 비교

|  | 풀 모델 | 푸시 모델 |
| --- | --- | --- |
| 손쉬운 디버깅 | 특정 엔드포인트를 두어 언제든 지표 데이터 조회 가능 → 풀 모델이 더 낫다 |  |
| 상태 진단 (health check) | 애플리케이션 서버가 풀 요청에 응답하지 않으면 장애가 발생한 것으로 인지
→ 풀 모델 쪽이 쉽다 | 네트워크 장애와 서버 장애 중 어떤 게 원인인지 알기 어려움 |
| 생존 기간이 짧은 프로세스 |  | 수집기가 지표를 끌어가기 전에 종료되어 버려도 push gateway 도입 등으로 해결 가능 → 푸시 모델이 더 낫다 |
| 방화벽 등 복잡한 네트워크 구성 | 모두 동일한 엔드포인트에 접근 가능하도록 구성. but, 여러 개의 데이터센터를 사용할 때 문제가 될 수 있어 세심한 설계가 필요함 | 수집기가 로드밸런서 및 Auto Scaling 클러스터 형태로 구성되어 있다면, 어디서 오든 모두 수집 가능 → 푸시 모델이 더 낫다 |
| 성능 | TCP 사용 ⇒ 오버헤드가 지표 데이터를 전송하는 것에 비해 낮다 | UDP 사용 ⇒ 지표 전송 지연이 더 낮다 |
| 데이터 신빙성 | 애플리케이션 서버는 정해져 있으므로 신뢰 가능 | 아무나 수집기에 데이터를 보낼 수 있으므로, 지표 전송 허용 목록을 수집기에서 들고 있거나 인증을 강제해야 해결 가능 |
</aside>

### 지표 전송 파이프라인의 규모 확장

![Untitled](https://github.com/user-attachments/assets/a2fc36c5-e948-4621-9348-7b8317e039a5)

> 스트림 처리 서비스(=큐의 소비자)가 큐 시스템(e.g. Kafka)과 시계열 DB 중간에 위치할 때 몇 가지 장점이 있다.
> 
> 
> *스트림 처리 서비스로는 아파치 스톰, 플링크, 스파크가 있다. 
> 
> *카프카는 고도로 안정적이고 규모 확장성이 뛰어난 분산 메시지 플랫폼이다. 
> 
- 데이터 수집 컴포넌트와 처리 컴포넌트 사이의 결합도를 낮춘다
- 데이터베이스에 장애가 생겨도 데이터는 소실되지 않는다 (카프카에 보관해두면 됨)

**[카프카를 통한 규모 확장]**

1. 대역폭 요구사항에 따라 파티션이 수를 설정한다
2. 지표 이름에 따라 어떤 지표를 어느 파티션에 배치할지 결정하면 소비자는 지표 이름에 따라 데이터를 집계할 수 있다
3. 태그/레이블에 따라 지표 데이터를 더욱 세분화한 파티션으로 나눈다
4. 중요 지표가 먼저 처리될 수 있도록 지표를 분류하고 우선순위를 지정한다

**[카프카의 대안]**

고릴라(Gorilla) : 페이스북의 메모리 기반 시계열 데이터베이스 시스템 

→ 큐 없이 대규모 데이터 처리가 가능한 모니터링 시스템

### 데이터 집계 지점

- **클라이언트 측** | `수집 에이전트` 가 집계하는 방안
    
    👎🏻 클라이언트 측에서는 복잡한 집계 로직을 지원하기 어려움. 어떤 카운터 값을 분 단위로 집계하여 지표 수집기에 보내는 정도 가능
    
- **저장소 기록 이전** | `데이터 수집 파이프라인` 에서 집계하는 방안
    
    → 플링크 같은 스트림 프로세싱 엔진 필요
    
    👍🏻 DB에는 계산 결과만 기록하므로, 실제로 기록되는 양은 엄청나게 줄어들 것
    
    👎🏻 늦게 도착하는 지포 데이터의 처리가 어렵고, 원본 데이터를 보관하지 않으므로 정밀도나 유연성 측면에서 손해를 보게 됨
    
- **저장소 기록 이후** | `질의 시` 에 집계하는 방안
    
    → 데이터를 날것 그대로 보관한 다음, 질의할 때 필요한 시간 구간에 맞게 집계
    
    👍🏻 데이터 손실 문제 X
    
    👎🏻 질의를 처리하는 순산에 전체 데이터세트를 대상으로 집계 결과를 계산해야 하므로 속도가 느림
    

### 질의 서비스

> 질의 서버 클러스터 형태로, 시각화 또는 경보 시스템에서 접수된 요청을 시계열 데이터베이스를 통해 처리하는 역할
> 

클라이언트 - 시계열 DB에 **질의 처리 전담 서비스**를 두면 결합도를 낮출 수 있다!

**`캐시 계층`** 

![IMG_8148.jpg](https://github.com/user-attachments/assets/ba06abfd-af5a-4c05-8671-61ade7ab1b04)

시계열 DB에 대한 질의 부하를 낮추고, 성능을 높일 수 있다

BUT.. 질의 서비스를 두면 곤란한 이유

- 상용 시각화 및 경보 시스템은 대부분 시계열 DB와의 연동을 처리하는 강력한 플러그인을 이미 갖추고 있는 경우가 많음
- 별도 캐시를 도입할 필요가 없는 시계열 DB가 있음

**`시계열 데이터베이스 질의어`**

프로메테우스, InfluxDB(→ Flux)에서는 SQL이 아닌 독자 질의어를 제공한다 (for 시계열 데이터 질의에 최적화)

ex. 지수 이동 평균 계산식

```sql
from(db:"telegraf")
	|> range(start:-1h)
	|> filter(fn: (r) => r._measurement == "foo")
	|> exponentialMovingAverage(size:-10s)
```

### 저장소 계층

1. **시계열 데이터베이스는 신중하게 선택할 것**
    
    성능을 크게 높이는 데 한 몫을 할 것이다.  
    
    ex. 운영 데이터 저장소에 대한 질의의 85%는 지난 26시간 내에 수집된 데이터를 대상으로 함 → 이를 잘 활용하는 InfluxDB 저장소 엔진을 선택함으로써 성능의 큰 이득을 봄
    
2. **저장 용량 최적화**
    - 데이터 인코딩 및 압축
    - 다운샘플링 : 데이터의 해상도를 낮춰 저장소 요구량을 줄이는 기법
        
        → 본 설계안의 보관 기간은 1년이므로 낡은 데이터의 해상도를 줄일 때 사용됨
        
    - 냉동 저장소 (cold storage) : 잘 사용되지 않는 비활성 상태 데이터를 보관하는 곳 (일반 저장소보다 비용 저렴)
    

### 경보 시스템

1. 설정 파일을 가져와 캐시 서버에 보관한다 (경보 규칙(주로 *.yaml)은 디스크에 파일 상태로 보관)
    
    ```yaml
    - name: instance_down
    rules:
    
    # Alert for any instance that is unreachable for >5
    - alert: instance_down
    		expr: up == 0
    		for: 5m
    		labels:
    		security: page
    ```
    
2. 경보 관리자는 경보 설정 내역을 캐시에서 가져온다
3. 설정된 규칙에 근거하여 경보 관리자는 지정된 시간마다 질의 서비스를 호출한다
    
    3-1. 질의 결과가 설정된 threshold를 위반하면 경보 이벤트 생성
    
    <aside>
    💡 경보 관리자가 수행하는 작업
    
    - 경보 필터링, 병합, 중복 제거 : 짧은 시간 동안 같은 인스턴스에서 발생한 경보를 병합(merge)할 수 있다
    - 접근 제어 : 사람의 실수에 의한 장애를 막고 시스템 보안을 유지하기 위해 특정 사용자로 제한
    - 재시도 : 경보 관리자는 경보 상태를 확인하고 알림이 최소 한 번은 전달됨을 보장
    </aside>
    
4. 경보 저장소는 카산드라 같은 형태의 키-값 저장소로 구현되며, 적어도 한 번 이상 알림이 전송되도록 보장한다
    
    *모든 경보의 상태(비활성화, 응답 대기, 경보 발령, 문제 해결 등)가 여기에 보관
    
5. 경보 이벤트를 카프카에 전달한다
6. 경보 소비자는 카프카에서 경보 이벤트를 읽는다
7. 경보 소비자는 카프카에서 읽은 경보 이벤트를 처리하여 이메일, 단문 메시지, PagerDuty, HTTP 서비스 엔드포인트 등의 다양한 채널로 알림을 전송한다

<aside>
🤔 경보 시스템 - 만들 것인가 구매할 것인가

이미 상용품이 널리 쓰이고 있고, 실무에서 경보 시스템을 밑바닥부터 구현하겠다는 아이디어는 수용되기 어렵다. 

</aside>

### 시각화 시스템

> 시각화 시스템은 데이터 계층 위에 만들어진다
> 
- 지표 대시보드 - 지표를 다양한 `시간 범위`로 표시
- 경보 대시보드 - 다양한 `경보의 상태`를 표시

→ [권장] 상용품을 구입해서 쓰는 것이 바람직하다 (ex. Grafana)

## 4단계 마무리

![Untitled](https://github.com/user-attachments/assets/e261f7f9-b8d1-4593-bd5f-62ef596c22c1)

- 지표 데이터 수집 모델 : 풀 모델 VS 푸시 모델
- 카프카를 활용한 규모 확장 방안
- 최적 시계열 데이터베이스의 선정
- 다운샘플링을 통한 데이터 크기 절감
- 경보/시각화 시스템 : 구현할 것인가 VS 구입할 것인가

### 요약

![IMG_8147.jpg](https://github.com/user-attachments/assets/4acd3cd8-1ac8-496d-9d04-766f3f27382c)

